# **LLD: Centralized Log Management & Automated Archival**

## **1. Problem Statement**

We need a **centralized log management system** to collect logs from multiple microservices, process them, and store them efficiently. The system should:

- Collect logs from **multiple microservices**.
- Use **Rolling File Appender** to manage log file sizes.
- Send logs to **Logstash**, which will process and forward them to **Elasticsearch**.
- Monitor **disk space usage** and send alerts if storage exceeds a threshold.
- Automatically archive old logs to **S3 Glacier** when disk space is low.

---

## **2. High-Level Architecture**

```
    [Microservices]  --->  [Rolling File Appender]  --->  [Logstash]
                                          |
                                          v
                             [Elasticsearch for Search]
                                          |
                                          v
                          [Kibana for Visualization]
                                          |
                                          v
                     [Prometheus Monitors Disk Space]
                                          |
                                          v
     [Alerting System] -----> [Triggers Auto-Archival to S3 Glacier]
```

---

## **3. Components & Responsibilities**

### **(a) Microservices Logging (SLF4J, Log4J2, Rolling File Appender)**

Each microservice writes logs to files using **Rolling File Appender**. Example configuration:

```properties
appender.rolling.type = RollingFile
appender.rolling.name = RollingFileAppender
appender.rolling.fileName = logs/app.log
appender.rolling.filePattern = logs/archive/app-%d{yyyy-MM-dd-HH}.log.gz
appender.rolling.policies.time.type = TimeBasedTriggeringPolicy
appender.rolling.policies.time.interval = 1
appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
appender.rolling.policies.size.size = 10MB
```

**Behavior:**

- Logs are saved to `logs/app.log`.
- Every hour (`%d{yyyy-MM-dd-HH}`), a new log file is created.
- Old logs are compressed (`.gz`) and moved to `logs/archive/`.
- If log file size exceeds `10MB`, a new file is created.

### **(b) Logstash Configuration (Sending Logs to Elasticsearch)**

Logstash processes log files and sends them to **Elasticsearch**.

```yaml
input {
file {
path => "/var/logs/app/*.log"
start_position => "beginning"
}
}
filter {
grok {
match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}" }
}
}
output {
elasticsearch {
hosts => ["http://elasticsearch:9200"]
index => "logs-%{+YYYY.MM.dd}"
}
}
```

### **(c) Kibana (Log Analysis & Visualization)**

- **Kibana Dashboard** visualizes logs stored in Elasticsearch.
- Provides filtering, searching, and error tracking.

### **(d) Prometheus Monitoring & Alerting for Disk Space**

**Prometheus Configuration (Monitoring Disk Usage)**:

```yaml
- job_name: "node"
  static_configs:
    - targets: ["node-exporter:9100"]
```

**PromQL Query for Alerting**:

```promql
(node_filesystem_avail_bytes{mountpoint="/var/log"} / node_filesystem_size_bytes{mountpoint="/var/log"}) * 100 < 10
```

Triggers an **alert when disk usage is above 90%**.

### **(e) Alertmanager (Triggering Auto-Archival to S3 Glacier)**

- Alertmanager **sends alerts** to an automation script when disk usage is critical.
- **Automation Script** moves logs older than 30 days to **S3 Glacier**.

**Cron Job for Log Archival to S3 Glacier**:

```bash
0 3 * * * find /var/log/app/ -type f -mtime +30 -exec aws s3 mv {} s3://my-app-logs-archive/ --storage-class GLACIER \;
```

**Explanation:**

- Runs **daily at 3 AM**.
- Moves logs **older than 30 days** to **S3 Glacier**.

---

## **4. Interview Answering Guide**

**Q1: How do you manage large log volumes?**
âœ… Use **Rolling File Appender** to limit log file size.
âœ… Use **Logstash** to centralize logs in **Elasticsearch**.
âœ… Store old logs in **S3 Glacier** to reduce disk usage.
âœ… Monitor disk space with **Prometheus**.

**Q2: How do you automate log archival?**
âœ… Use **cron jobs** or **Fluentd** to upload logs older than 30 days to **S3 Glacier**.
âœ… Alerts from **Prometheus** trigger automatic archival.

**Q3: How do you monitor and alert on disk space?**
âœ… **Prometheus** tracks disk usage.
âœ… If space is **above 90%**, **Alertmanager** triggers an action.
âœ… Automation moves logs to **cold storage**.

---

## **5. Summary**

ðŸš€ **Microservices generate logs** â†’ Rolling File Appender organizes them.
ðŸš€ **Logstash sends logs to Elasticsearch** â†’ Kibana provides search & visualization.
ðŸš€ **Prometheus monitors disk usage** â†’ Alertmanager triggers log archival.
ðŸš€ **Old logs are moved to S3 Glacier** for cost savings.

This setup ensures **scalability, cost efficiency, and real-time monitoring** of logs. ðŸ”¥

//////////////////
Q - Is node exporter doing it getting the info?
Yes! Node Exporter is responsible for collecting infrastructure-level metrics like CPU, memory, disk usage, network stats, etc., from each node in a Kubernetes cluster.

For your log volume management LLD, Node Exporter will:
âœ… Monitor disk space usage on nodes.
âœ… Provide real-time disk utilization metrics to Prometheus.
âœ… Enable Prometheus Alertmanager to send alerts if disk usage crosses a threshold (e.g., 80% full).

How It Works in Your LLD:
1ï¸âƒ£ Microservices Write Logs â†’ Using RollingFileAppender (Log4j/SLF4J).
2ï¸âƒ£ Fluentd or Logstash Collect Logs â†’ Send to Elasticsearch (for search) or S3 Glacier (for cold storage).
3ï¸âƒ£ Node Exporter Collects Disk Metrics â†’ Sends to Prometheus.
4ï¸âƒ£ Prometheus Monitors & Triggers Alerts â†’ If disk usage exceeds limits, triggers Alertmanager.
5ï¸âƒ£ Alertmanager Notifies DevOps â†’ Email, Slack, PagerDuty, etc.

Do You Need a Diagram?
I can create one to visually map this flow! ðŸš€

# Log Volume Management LLD

## 1ï¸âƒ£ Problem Statement

- Microservices generate large volumes of logs.
- Logs should be **rotated, archived, and moved** to cold storage when not needed.
- Prevent disk space from filling up by **monitoring & alerting** when usage is high.

## 2ï¸âƒ£ Components

### **Log Collection & Storage**

1. **Microservices** â†’ Generate logs using **RollingFileAppender (Log4j, SLF4J)**.
2. **Fluentd / Logstash** â†’ Collect logs & send to a **centralized server** (e.g., Elasticsearch).
3. **Elasticsearch** â†’ Index logs for **fast searching**.
4. **S3 Glacier (Cold Storage)** â†’ Store old logs based on a **retention policy**.

### **Monitoring & Alerting**

5. **Node Exporter** â†’ Collects **disk usage** metrics from Kubernetes nodes.
6. **Prometheus** â†’ Scrapes metrics from Node Exporter.
7. **Alertmanager** â†’ Triggers alerts if **disk usage exceeds threshold** (e.g., 80%).
8. **Notification System** â†’ Sends alerts to **Slack, PagerDuty, Email**, etc.

## 3ï¸âƒ£ Solution Flow

1ï¸âƒ£ Microservices generate logs and store them in rolling log files.  
2ï¸âƒ£ Logstash/Fluentd sends logs to **Elasticsearch** for indexing.  
3ï¸âƒ£ Older logs are **archived to S3 Glacier** using an automated retention policy.  
4ï¸âƒ£ **Node Exporter monitors disk usage** and reports to **Prometheus**.  
5ï¸âƒ£ If **disk usage exceeds 80%**, **Alertmanager** triggers an alert.  
6ï¸âƒ£ DevOps team gets notified via **Slack, PagerDuty, or Email**.

## 4ï¸âƒ£ Technologies Used

âœ… **Log Collection** â†’ Log4j, SLF4J, Fluentd, Logstash  
âœ… **Log Storage** â†’ Elasticsearch, S3 Glacier  
âœ… **Monitoring** â†’ Prometheus, Node Exporter  
âœ… **Alerting** â†’ Alertmanager, Grafana  
âœ… **Cold Storage** â†’ AWS S3 Glacier (for long-term logs)

## 5ï¸âƒ£ Example Prometheus Alert Rule

```yaml
- alert: HighDiskUsage
  expr: node_filesystem_avail_bytes / node_filesystem_size_bytes * 100 < 20
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "Disk space running low"
    description: "The disk usage is above 80%, needs cleanup."
```

Automation & Scaling
Lifecycle Policies: Automatically move logs from Elasticsearch â†’ S3 Glacier.

Scheduled Cleanup Jobs: Delete logs older than X days from Elasticsearch.

Auto-scaling EKS nodes: If logs flood the system, scale nodes automatically.

âœ… Summary
Logs are collected â†’ Sent to Elasticsearch.

Old logs move to cold storage â†’ S3 Glacier.

Disk usage is monitored â†’ Node Exporter + Prometheus.

Alerts notify DevOps â†’ If space is low, trigger cleanup or scale nodes.
